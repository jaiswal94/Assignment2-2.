1>
FS-
It stands for Hadoop Distributed File System which is the primary storage used by hadoop aplication.
it provides high perfomance access to data across hadoop cluster.
It is a tool for managing pools of big data and their supporting analytics.
it is deployed on low-cost commidity hardware,server failures occur.The file system is designed to be highly fault-tolerant,
however, by facilitating the rapid transfer of data between compute nodes and enabling Hadoop systems to continue running 
if a node fails which decreases the risk of catastrophic failure.

when data is taken by hadoop,the information gets broken into seperate pieces and ditribute them to different 
node in acluster,allowing parallel processing.The file system also copies each piece of data multiple times and
distributes the copies to individual nodes, placing at least one copy on a different server rack than the others.
As a result, the data on nodes that crash can be found elsewhere within a cluster, which allows processing to 
continue while the failure is resolved.
HDFS use a  master/slave architecture, with each cluster consisting of a single NameNode that manages file 
system operations and supporting DataNodes that manage data storage on individual compute nodes.







2> 
Hadoop cluster is a special type of computational cluster designed specifically for storing and analyzing huge amounts of 
unstructured data in a distributed computing environment. Hadoop clusters run Hadoop's open source distributed processing software 
on low-cost commodity computers. one machine in the cluster is designated as the NameNode and another machine the as JobTracker; 
these are the masters.The rest of the machines in the cluster act as both DataNode and TaskTracker; these are the slaves. 
Hadoop clusters are known for boosting the speed of data analysis applications. They also are highly scalable: If a cluster's
processing power is overwhelmed by growing volumes of data, additional cluster nodes can be added to increase throughput.
Hadoop clusters also are highly resistant to failure because each piece of data is copied onto other cluster nodes, 
which ensures that the data is not lost if one node fails.
Hadoop clusters are known for boosting the speed of data analysis applications. They also are highly scalable:
If a cluster's processing power is overwhelmed by growing volumes of data, additional cluster nodes can be added 
to increase throughput. Hadoop clusters also are highly resistant to failure because each piece of data is copied
onto other cluster nodes, which ensures that the data is not lost if one node fails.




3>HDFS Blocks-
A block is the smallest unit of data that can be stored or retrieved from the disk. 
Filesystems deal with the data stored in blocks. Filesystem blocks are normally in 
few kilobytes of size. Blocks are transparent to the user who is performing filesystem operations like read and write.
It stores the data in terms of blocks. However the block size in HDFS is very large.
The default size of HDFS block is 64MB. The files are split into 64MB blocks and then stored
into the hadoop filesystem. The hadoop application is responsible for distributing the data blocks across multiple nodes. 


